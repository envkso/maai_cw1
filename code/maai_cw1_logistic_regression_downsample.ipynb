{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from sklearn.utils.validation import column_or_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANY PARAMETERS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptDir = os.getcwd() # Assumes that script is executed from its actual location\n",
    "relPath = r\"../output/\" # And data is located in a 'data' folder in the same parent directory as 'code' folder\n",
    "XtrainFilePath = os.path.join(scriptDir, relPath, \"X_train_pp.csv\")\n",
    "ytrainFilePath = os.path.join(scriptDir, relPath, \"y_train.csv\")\n",
    "XvalFilePath = os.path.join(scriptDir, relPath, \"X_val_pp.csv\")\n",
    "yvalFilePath = os.path.join(scriptDir, relPath, \"y_val.csv\")\n",
    "XtestFilePath = os.path.join(scriptDir, relPath, \"X_test_pp.csv\")\n",
    "columnsNamesPath = os.path.join(scriptDir, relPath, \"column_names.csv\")\n",
    "\n",
    "relPathOutput = r\"../output/\"\n",
    "outputFolderPath = os.path.join(scriptDir, relPathOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.read_csv(columnsNamesPath,header=None)\n",
    "columns = list(columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features data\n",
    "X_train = pd.read_csv(XtrainFilePath, delimiter=',',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2430981, 547)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check expected dimensions\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels\n",
    "y_train = pd.read_csv(ytrainFilePath, delimiter=',',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1793"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many positive labels?\n",
    "n = y_train[0].sum()\n",
    "n\n",
    "# this is how many samples we'll take from data where the label = 0\n",
    "# giving a dataset containing 50% 1s, and 50% 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling\n",
    "# take all the records with label = 1\n",
    "X_train_1 = X_train[y_train[0] == 1]\n",
    "y_train_1 = y_train[y_train[0] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get n random records with label = 0\n",
    "idx = y_train[y_train[0] == 0].sample(n=n).index\n",
    "X_train_0 = X_train.loc[idx]\n",
    "y_train_0 = y_train.loc[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine to create training set\n",
    "X_train_ds = pd.concat([X_train_0, X_train_1])\n",
    "y_train_ds = pd.concat([y_train_0, y_train_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3586, 547)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check sizes\n",
    "X_train_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3586, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# SINGLE PASS EXAMPLE\n",
    "clf = LogisticRegression(solver='liblinear').fit(X_train_ds, y_train_ds[0].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/logRes_2_liblinear_l2_ds_20190228.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "dump(clf, r\"../output/logRes_2_liblinear_l2_ds_20190228.joblib\")\n",
    "# clf = load(r\"../output/xxx.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'warn',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'liblinear',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params() # check params of loaded model, confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on the validation data\n",
    "X_val = pd.read_csv(XvalFilePath, delimiter=',',header=None)\n",
    "y_val = pd.read_csv(yvalFilePath, delimiter=',',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_val)\n",
    "predictionProb = clf.predict_proba(X_val) # note this gives probability for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4849189767212306 0.5691361132438844 0.0008428688189619942 0.6534653465346535 0.0016835660991008224 147247 156476 70 132\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "bAccuracy = balanced_accuracy_score(y_val, predictions)\n",
    "precision = precision_score(y_val, predictions)\n",
    "recall = recall_score(y_val, predictions)\n",
    "f1 = f1_score(y_val, predictions)\n",
    "tn, fp, fn, tp = np.reshape(confusion_matrix(y_val, predictions), (4,))\n",
    "print(accuracy, bAccuracy, precision, recall, f1, tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547 499\n"
     ]
    }
   ],
   "source": [
    "# how many zeros / non-zeros do we have?\n",
    "# total attributes, and non-zero count\n",
    "print(clf.coef_.shape[1], np.where(clf.coef_ == 0.0, 0, 1).sum())\n",
    "# TODO check coefficients against the full column list - see what is zero (may remove later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Repeated random down-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 100\n",
    "scores = np.zeros((repeats, 5))\n",
    "predictionsProbAll = np.zeros((y_val.shape[0], repeats))\n",
    "for i in range(repeats):\n",
    "    # random downsample\n",
    "    idx = y_train[y_train[0] == 0].sample(n=n).index\n",
    "    X_train_0 = X_train.loc[idx]\n",
    "    y_train_0 = y_train.loc[idx]\n",
    "    X_train_ds = pd.concat([X_train_0, X_train_1])\n",
    "    y_train_ds = pd.concat([y_train_0, y_train_1])\n",
    "    # train, predict, score\n",
    "    clf = LogisticRegression(solver='liblinear' , penalty='l2').fit(X_train_ds, y_train_ds.ravel())\n",
    "#     predictions = clf.predict(X_val)\n",
    "    predictionsProb = clf.predict_proba(X_val)\n",
    "    predictionsProbAll[:,i] = predictionsProb[:,1] # just prob for label=1\n",
    "#     accuracy = accuracy_score(y_val, predictions)\n",
    "#     bAccuracy = balanced_accuracy_score(y_val, predictions)\n",
    "#     precision = precision_score(y_val, predictions)\n",
    "#     recall = recall_score(y_val, predictions)\n",
    "#     f1 = f1_score(y_val, predictions)\n",
    "#     scores[i, :] = np.array([accuracy, bAccuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take average predicted probability\n",
    "predictionsProbMean = np.mean(predictionsProbAll,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this into class predictions using standard threshold of 0.5\n",
    "predictionsMean = np.where(predictionsProbMean>0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5730624331660772 0.5810803359353329 0.000916844513956839 0.5891089108910891 0.001830839647678757 174049 129674 83 119\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics\n",
    "accuracy = accuracy_score(y_val, predictionsMean)\n",
    "bAccuracy = balanced_accuracy_score(y_val, predictionsMean)\n",
    "precision = precision_score(y_val, predictionsMean)\n",
    "recall = recall_score(y_val, predictionsMean)\n",
    "f1 = f1_score(y_val, predictionsMean)\n",
    "tn, fp, fn, tp = np.reshape(confusion_matrix(y_val, predictionsMean), (4,))\n",
    "print(accuracy, bAccuracy, precision, recall, f1, tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce probabilities for the testing set\n",
    "X_test = pd.read_csv(XtestFilePath, delimiter=',',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "predictionProb = clf.predict_proba(X_test) # note this gives probability for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(r\"../output/logRes_2_liblinear_l2_ds_20190228_OUT.csv\", predictionProb[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
